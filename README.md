# EmbeddingViz

This repository demonstrates using a cold-start recommendation model trained
using ThirdAI's offering for a product search application. We pick up here after running the training, and
saving embedding for products following the example in [the cold-start
recommendation demo
notebook](https://github.com/ThirdAILabs/Demos/blob/b22556950bd23ae3e3873d1b4bff808805bc941b/EmbeddingsAndColdStart.ipynb).

<img src="./images/galaxy-render.png">

## Source organization

The source is organized as follows.

```bash
cold_start/ # Using `thirdai` for inference and datagen (Python)
galaxy/     # Visualization frontend (mostly JS)
layout/     # layout algorithm for graph (C++)
data/       # Contains the manifest file and other data¯
```


## Dependencies
We integrate the product search using query from an available catalog to the
search-bar for purposes of a demonstration. The frontend view is a single-page
app, adapted from [anvaka/pm](https://github.com/anvaka/pm). The backend is
built with FastAPI and runs inference using models built using `thirdai`.

**Backend** Setup involves installing `fastapi` and `uvicorn`, and using a
virtual-environment is recommended.

```bash
python3 -m venv env
. env/bin/activate
python3 -m pip install fastapi uvicorn
```

**Frontend** Development follows [anvaka/pm](https://github.com/anvaka/pm) to
use the node ecosystem. For the development requirements, please do a
clean-install via npm, which will fetch dependencies from
[package.json](galaxy/package.json):

```bash
cd galaxy
sudo apt-get install npm
```

## Manual Setup
This section goes over how you can manually setup the galaxy app for serving your data. For automatic setup, go to Automatic Setup Section.
### Generating data

Data can be generated from an existing embedding save using [generate_graph.py](./cold_start/generate_graph.py).

```bash
# Tweak neighbours and threshold to get a good force-layout.
python3 cold_start/generate_graph.py --catalog_path /path-to-catalog \
    --embed-path /path/to/embed                                 \
    --output-dir /path/to/output-dir                            \
    --neighbours 20 --threshold 20
    --version 1 --strong_column_names ["Title"]
    --target_name "Id" --indexer faiss --undirected_edges False
```

Note: Manual setup works only when the indexer is faiss. Due to a large number of hyperparameters used for model indexing, automatic setup should be used for model indexing. 

This will create `links.bin` and `labels.json` in `/path/to/output-dir/v{version}`. This
is consumed by the visualization. More details on how to organize this is given
below.

### Serving data

Data is expected to be served from a static source. In the cold_start/cold_start_fastapi.py file, we load the data folder with the address as `/api_prefix/data`. The `dataUrl` becomes `/api_prefix/` and this has to be
set in [config.js](galaxy/src/config.js) as `dataUrl` so that the galaxy app can load data from the directory and serve it. A visualization
requires the following files, taking the example of `data`:

```bash
data
├── manifest.json
├── v1
│   ├── labels.json    # holds labels (string Ids)
│   ├── links.bin      # Binary file holding link information
│   ├── meta.json      # Some metadata.
│   ├── positions.bin  # Positions computed by `layout` using `links.bin`
```

For more information see
[anvaka/pm#data-format](https://github.com/anvaka/pm#data-format).


### Layout generation

The force-layout that is required to render the Embedding visualization is
created by using [layout](layout/), which is written in C++ and requires OpenMP.
This is adapted from
[anvaka/ngraph.native](https://github.com/anvaka/ngraph.native).

To get the executable `layout` which consumes `links.bin` generated by the
python script to generate `positions.bin`, compile from source using `cmake`:

```bash
cd layout;
mkdir build; cd build;
cmake ..
make -j3
```

For `data` with the directly structure described before. This process
can take a while, depending on the size of your graph.

```bash
./layout data/v1/links.bin
mv positions.bin data/v1/positions.bin 
```

### Starting the server

Once the graph and the layout has been generated, we can build our frontend. We first have to modify the [config.js](galaxy/src/config.js) file. 

The variables in the config file are: 
1. dataUrl : `/api_prefix/`
2. apiUrl  : `/api_prefix/predict`
3. target_name
4. strong_column_names
5. description : Description of the visualization
6. name : The name of the visualization 
   
Once config file has been modified, run :
```bash
cd galaxy
npm ci
```

Following building the frontend, a development server can be spawned by running:

```bash
cd cold_start 
MODEL_PATH=/path/to/model CATALOG_PATH=/path/to/catalog uvicorn cold_start_fastapi:app 
```

The predict api is loaded onto the endpoint `/api_prefix/predict`

The app `cold_start_fastapi:app` is the app responsible for serving the data, getting predictions from the model, and visualization for the users.


## Automatic Setup
Installing the dependencies is a pre-req for this section. 

This is purely a config driven approach for setup. 

Make a file called config.json that has the following elements:
1. catalog_path
2. model_path
3. js_config_path : path to frontend config.js file ( galaxy/src/config.js ). 
4. output_dir : Should follow the directory structure as mentioned at the top
5. version
6. galaxy_dir : galaxy
7. layout_dir : layout
8. name
9. description
10. api_prefix
11. query_column_name
12. target_name
13. strong_column_names
14. indexer : model or faiss
15. undirected_edges : true or false
16. k_grams : int or None
17. columns_to_combine : list of columns if indexer is model. These columns are concatenanted and then passed to model for inference 
18. sparse_inference : true or false if indexer is model. else not needed
19. neighbours 
20. threshold
20. embed_path

In the setup.sh file, set the value of the variable `CONFIG_PATH`. Run the setup file.

```bash
cd cold_start
python3 setup.py
uvicorn cold_start_fastapi:app --reload --port 80xx
```

This will generate the graph and the layout for the file, build the frontend, and run the visualization server. 


## Exposing the port using nginx

Go to /etc/nginx/sites-available/default and add 

```json
location /api_prefix {
		proxy_pass http://127.0.0.1:80xx/api_prefix;
	}
```
tp the file and then 
```bash
sudo systemctl restart nginx
```

This will expose your app to TCP traffic.